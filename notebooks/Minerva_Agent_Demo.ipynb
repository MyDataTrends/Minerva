{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minerva Agent Workforce â€” Full Demo Notebook\n",
    "\n",
    "This notebook demonstrates every agent in the Minerva system, triggers each one to do real work,\n",
    "and shows how the **Conductor** orchestrates the whole workforce into a daily digest.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CONDUCTOR (Orchestrator)                  â”‚\n",
    "â”‚  Runs daily â€” delegates to sub-agents â€” compiles digest     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚        â”‚        â”‚        â”‚        â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚Engineer â”‚ â”‚Sentinelâ”‚ â”‚Advocateâ”‚ â”‚Telemetryâ”‚ â”‚Productizerâ”‚\n",
    "    â”‚Gap Ana- â”‚ â”‚QA Gate-â”‚ â”‚GitHub â”‚ â”‚Usage   â”‚ â”‚Sales Kits â”‚\n",
    "    â”‚lysis    â”‚ â”‚keeper  â”‚ â”‚Triage â”‚ â”‚Metrics â”‚ â”‚Generator  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚Marketing â”‚ â”‚Support  â”‚ â”‚Presentation â”‚\n",
    "    â”‚Social    â”‚ â”‚FAQ Bot  â”‚ â”‚PowerPoint   â”‚\n",
    "    â”‚Drafts    â”‚ â”‚         â”‚ â”‚Generator    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**All agents run in `dry_run=True` mode** â€” they report what they would do without side effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup â€” Add Project Root to Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# In Jupyter, __file__ doesn't exist. Use cwd-based detection.\n# This notebook lives in notebooks/ â†’ project root is the parent.\n_notebook_dir = Path(os.path.abspath(\"\"))\nPROJECT_ROOT = _notebook_dir.parent if _notebook_dir.name == \"notebooks\" else _notebook_dir\n\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.insert(0, str(PROJECT_ROOT))\n\n# Also load .env if present\ntry:\n    from dotenv import load_dotenv\n    load_dotenv(PROJECT_ROOT / \".env\", override=False)\n    print(\"âœ… .env loaded\")\nexcept ImportError:\n    print(\"â„¹ï¸  python-dotenv not installed â€” .env not loaded\")\n\n# Verify LLM availability\nfrom llm_manager.llm_interface import is_llm_available, get_active_model_name\nllm_ok = is_llm_available()\nprint(f\"{'âœ…' if llm_ok else 'âš ï¸ '} LLM available: {llm_ok} â€” active model: {get_active_model_name()}\")\nprint(f\"ğŸ“ Project root: {PROJECT_ROOT}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Generate Demo Data\n",
    "\n",
    "We create a synthetic **retail e-commerce** dataset that several agents will use.\n",
    "This mimics a real upload without needing external data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# â”€â”€ Synthetic Retail Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n = 500\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home & Garden\", \"Sports\", \"Books\"]\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "channels = [\"Online\", \"In-Store\", \"Mobile App\"]\n",
    "\n",
    "dates = [datetime(2025, 1, 1) + timedelta(days=random.randint(0, 364)) for _ in range(n)]\n",
    "\n",
    "df_retail = pd.DataFrame({\n",
    "    \"order_id\":        range(10001, 10001 + n),\n",
    "    \"order_date\":      [d.strftime(\"%Y-%m-%d\") for d in dates],\n",
    "    \"customer_id\":     np.random.randint(1000, 2000, n),\n",
    "    \"product_category\": np.random.choice(categories, n, p=[0.30, 0.25, 0.20, 0.15, 0.10]),\n",
    "    \"revenue\":         np.round(np.random.lognormal(4.5, 0.8, n), 2),\n",
    "    \"units_sold\":      np.random.randint(1, 15, n),\n",
    "    \"discount_pct\":    np.round(np.random.uniform(0, 0.35, n), 2),\n",
    "    \"region\":          np.random.choice(regions, n),\n",
    "    \"channel\":         np.random.choice(channels, n, p=[0.55, 0.30, 0.15]),\n",
    "    \"customer_rating\": np.random.choice([1, 2, 3, 4, 5], n, p=[0.03, 0.07, 0.15, 0.35, 0.40]),\n",
    "    \"is_returned\":     np.random.choice([0, 1], n, p=[0.88, 0.12]),\n",
    "    \"days_to_ship\":    np.random.randint(1, 8, n),\n",
    "})\n",
    "\n",
    "# Save to User_Data (for Productizer)\n",
    "user_data_dir = PROJECT_ROOT / \"User_Data\"\n",
    "user_data_dir.mkdir(exist_ok=True)\n",
    "demo_csv_path = user_data_dir / \"demo_retail_sales.csv\"\n",
    "df_retail.to_csv(demo_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ… Created demo dataset: {demo_csv_path}\")\n",
    "print(f\"   Shape: {df_retail.shape}\")\n",
    "print(f\"   Columns: {list(df_retail.columns)}\")\n",
    "df_retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LLM Interface Demo\n",
    "\n",
    "Before we trigger agents, let's see the LLM layer in action â€” analyzing data and suggesting visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_manager.llm_interface import analyze_data_with_llm, suggest_visualizations, get_llm_completion\n",
    "\n",
    "print(\"â•\" * 60)\n",
    "print(\"  LLM INTERFACE: Data Analysis\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "question = \"Which product category drives the most revenue, and are there any return rate patterns I should worry about?\"\n",
    "print(f\"\\nQuestion: {question}\\n\")\n",
    "\n",
    "if is_llm_available():\n",
    "    analysis = analyze_data_with_llm(df_retail, question)\n",
    "    print(analysis if analysis else \"[No response â€” LLM may need configuration]\")\n",
    "else:\n",
    "    # Show what the prompt looks like\n",
    "    print(\"âš ï¸  No LLM configured. Here's a preview of what Minerva would send:\")\n",
    "    stats_summary = df_retail.describe().to_string()\n",
    "    sample = df_retail.head(3).to_string()\n",
    "    print(f\"\\nDataset: {df_retail.shape[0]} rows Ã— {df_retail.shape[1]} cols\")\n",
    "    print(f\"Key stats: Total revenue = ${df_retail['revenue'].sum():,.2f}\")\n",
    "    by_cat = df_retail.groupby('product_category')['revenue'].sum().sort_values(ascending=False)\n",
    "    print(f\"Top category by revenue: {by_cat.index[0]} (${by_cat.iloc[0]:,.2f})\")\n",
    "    return_rates = df_retail.groupby('product_category')['is_returned'].mean()\n",
    "    print(f\"Highest return rate: {return_rates.idxmax()} ({return_rates.max():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  LLM INTERFACE: Visualization Suggestions\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "if is_llm_available():\n",
    "    suggestions = suggest_visualizations(df_retail)\n",
    "    print(f\"\\nSuggested {len(suggestions)} visualizations:\\n\")\n",
    "    for i, s in enumerate(suggestions, 1):\n",
    "        print(f\"{i}. [{s.get('type','?').upper()}] {s.get('title','Untitled')}\")\n",
    "        print(f\"   X: {s.get('x','?')}  |  Y: {s.get('y','?')}\")\n",
    "        if s.get('reason'):\n",
    "            print(f\"   Why: {s.get('reason')}\")\n",
    "        print()\n",
    "else:\n",
    "    # Deterministic fallback suggestions\n",
    "    suggestions = [\n",
    "        {\"type\": \"bar\", \"x\": \"product_category\", \"y\": \"revenue\", \"title\": \"Revenue by Category\"},\n",
    "        {\"type\": \"line\", \"x\": \"order_date\", \"y\": \"revenue\", \"title\": \"Revenue Over Time\"},\n",
    "        {\"type\": \"scatter\", \"x\": \"discount_pct\", \"y\": \"revenue\", \"title\": \"Discount vs Revenue\"},\n",
    "    ]\n",
    "    print(\"\\n[No LLM â€” using heuristic suggestions]\")\n",
    "    for i, s in enumerate(suggestions, 1):\n",
    "        print(f\"{i}. [{s['type'].upper()}] {s['title']} â€” X:{s['x']} / Y:{s['y']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the suggested charts\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Chart 1: Revenue by category\n",
    "cat_revenue = df_retail.groupby('product_category')['revenue'].sum().reset_index().sort_values('revenue', ascending=True)\n",
    "fig1 = px.bar(cat_revenue, x='revenue', y='product_category', orientation='h',\n",
    "              title='Total Revenue by Product Category',\n",
    "              color='revenue', color_continuous_scale='Viridis',\n",
    "              labels={'revenue': 'Total Revenue ($)', 'product_category': 'Category'})\n",
    "fig1.show()\n",
    "\n",
    "# Chart 2: Return rate + avg rating by category\n",
    "cat_stats = df_retail.groupby('product_category').agg(\n",
    "    return_rate=('is_returned', 'mean'),\n",
    "    avg_rating=('customer_rating', 'mean'),\n",
    "    orders=('order_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "fig2 = make_subplots(rows=1, cols=2, subplot_titles=['Return Rate by Category', 'Avg Rating by Category'])\n",
    "fig2.add_trace(go.Bar(x=cat_stats['product_category'], y=cat_stats['return_rate'],\n",
    "                      name='Return Rate', marker_color='#ef5350'), row=1, col=1)\n",
    "fig2.add_trace(go.Bar(x=cat_stats['product_category'], y=cat_stats['avg_rating'],\n",
    "                      name='Avg Rating', marker_color='#42a5f5'), row=1, col=2)\n",
    "fig2.update_layout(title='Quality Metrics by Category', showlegend=False)\n",
    "fig2.show()\n",
    "\n",
    "# Chart 3: Revenue by channel over time (monthly)\n",
    "df_retail['month'] = pd.to_datetime(df_retail['order_date']).dt.to_period('M').astype(str)\n",
    "monthly_channel = df_retail.groupby(['month', 'channel'])['revenue'].sum().reset_index()\n",
    "fig3 = px.line(monthly_channel, x='month', y='revenue', color='channel',\n",
    "               title='Monthly Revenue by Sales Channel',\n",
    "               labels={'revenue': 'Revenue ($)', 'month': 'Month'})\n",
    "fig3.show()\n",
    "\n",
    "print(\"âœ… Charts rendered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Agent Setup â€” Load All Agents\n",
    "\n",
    "We instantiate every agent in dry-run mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.base import AgentConfig, AgentResult, Priority\n",
    "from agents.conductor import ConductorAgent\n",
    "from agents.engineer import EngineerAgent\n",
    "from agents.sentinel import SentinelAgent\n",
    "from agents.advocate import AdvocateAgent\n",
    "from agents.telemetry import TelemetryAgent\n",
    "from agents.productizer import ProductizerAgent\n",
    "from agents.marketing import MarketingAgent\n",
    "from agents.support import SupportAgent\n",
    "from agents.presentation import PresentationAgent\n",
    "\n",
    "def make_config(name, enabled=True, dry_run=True, extra=None):\n",
    "    return AgentConfig(\n",
    "        name=name,\n",
    "        enabled=enabled,\n",
    "        dry_run=dry_run,\n",
    "        llm_model=\"claude-3-5-sonnet\",\n",
    "        extra=extra or {}\n",
    "    )\n",
    "\n",
    "# Instantiate all agents\n",
    "agents = {\n",
    "    \"engineer\":     EngineerAgent(config=make_config(\"engineer\")),\n",
    "    \"sentinel\":     SentinelAgent(config=make_config(\"sentinel\", extra={\"min_confidence_score\": 7})),\n",
    "    \"advocate\":     AdvocateAgent(config=make_config(\"advocate\", extra={\"stale_days\": 14})),\n",
    "    \"telemetry\":    TelemetryAgent(config=make_config(\"telemetry\")),\n",
    "    \"productizer\":  ProductizerAgent(config=make_config(\"productizer\")),\n",
    "    \"marketing\":    MarketingAgent(config=make_config(\"marketing\")),\n",
    "    \"support\":      SupportAgent(config=make_config(\"support\")),\n",
    "    \"presentation\": PresentationAgent(config=make_config(\"presentation\")),\n",
    "}\n",
    "\n",
    "print(\"Minerva Agent Workforce\")\n",
    "print(\"â•\" * 55)\n",
    "emoji_map = {\n",
    "    \"engineer\": \"ğŸ”§\", \"sentinel\": \"ğŸ›¡ï¸\", \"advocate\": \"ğŸ¤\",\n",
    "    \"telemetry\": \"ğŸ“Š\", \"productizer\": \"ğŸ’¼\", \"marketing\": \"ğŸ“£\",\n",
    "    \"support\": \"ğŸ’¬\", \"presentation\": \"ğŸ“‘\",\n",
    "}\n",
    "role_map = {\n",
    "    \"engineer\": \"Codebase gap analysis & improvement recommender\",\n",
    "    \"sentinel\": \"QA gatekeeper â€” tests, lint, confidence scoring\",\n",
    "    \"advocate\": \"GitHub issue triage & community management\",\n",
    "    \"telemetry\": \"Usage analytics & health monitoring\",\n",
    "    \"productizer\": \"Vertical sales kit generator\",\n",
    "    \"marketing\": \"Social media draft generator from git log\",\n",
    "    \"support\": \"FAQ-backed intelligent issue responder\",\n",
    "    \"presentation\": \"PowerPoint generator from sales kits\",\n",
    "}\n",
    "for name, agent in agents.items():\n",
    "    e = emoji_map.get(name, \"â€¢\")\n",
    "    mode = \"[dry-run]\" if agent.is_dry_run else \"[LIVE]\"\n",
    "    print(f\"  {e} {name:14s} {mode}  {role_map.get(name, '')}\")\n",
    "print()\n",
    "print(f\"  All {len(agents)} agents ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Engineer Agent â€” Codebase Gap Analysis\n",
    "\n",
    "The Engineer reads subsystem source files and scores their maturity against best-in-class standards.\n",
    "It writes the report to `knowledge_base/product/vision_gap_analysis.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"â•\" * 60)\n",
    "print(\"  ENGINEER AGENT â€” Gap Analysis\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "engineer_result = agents[\"engineer\"].run()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if engineer_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if engineer_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {engineer_result.summary}\")\n",
    "\n",
    "if engineer_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in engineer_result.metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"   {k}: {v:.2f}\")\n",
    "        else:\n",
    "            print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in engineer_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if engineer_result.escalations:\n",
    "    print(\"\\nâš ï¸  Escalations:\")\n",
    "    for esc in engineer_result.escalations:\n",
    "        emoji = {\"urgent\": \"ğŸ”´\", \"review\": \"ğŸŸ¡\", \"fyi\": \"ğŸŸ¢\", \"metric\": \"ğŸ“Š\"}.get(esc.priority.value, \"â€¢\")\n",
    "        print(f\"   {emoji} [{esc.priority.value.upper()}] {esc.title}\")\n",
    "        print(f\"      {esc.detail[:200]}\")\n",
    "\n",
    "if engineer_result.error:\n",
    "    print(f\"\\nâš ï¸  Error: {engineer_result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated gap analysis report\n",
    "from pathlib import Path\n",
    "\n",
    "gap_report_path = PROJECT_ROOT / \"agents\" / \"knowledge_base\" / \"product\" / \"vision_gap_analysis.md\"\n",
    "if gap_report_path.exists():\n",
    "    from IPython.display import Markdown, display\n",
    "    content = gap_report_path.read_text(encoding=\"utf-8\")\n",
    "    print(f\"ğŸ“„ Gap analysis report ({len(content)} chars):\")\n",
    "    display(Markdown(content))\n",
    "else:\n",
    "    print(\"(Gap analysis report not yet written â€” LLM may be unavailable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize subsystem maturity scores\n",
    "import plotly.express as px\n",
    "\n",
    "# Engineer's SUBSYSTEMS with their key files\n",
    "subsystems = {\n",
    "    \"cascade_planner\": \"orchestration/cascade_planner.py\",\n",
    "    \"plan_learner\": \"orchestration/plan_learner.py\",\n",
    "    \"mcp_server\": \"mcp_server/server.py\",\n",
    "    \"api_discovery\": \"mcp_server/discovery_agent.py\",\n",
    "    \"dynamic_connectors\": \"mcp_server/dynamic_connector.py\",\n",
    "    \"model_training\": \"modeling/model_training.py\",\n",
    "    \"smart_charts\": \"visualization/smart_charts.py\",\n",
    "    \"nl_query\": \"ui/nl_query.py\",\n",
    "    \"data_fabric\": \"ui/data_fabric.py\",\n",
    "    \"dashboard\": \"ui/dashboard.py\",\n",
    "}\n",
    "\n",
    "# Count lines as a quick proxy for maturity\n",
    "maturity_data = []\n",
    "for name, filepath in subsystems.items():\n",
    "    full_path = PROJECT_ROOT / filepath\n",
    "    if full_path.exists():\n",
    "        lines = len(full_path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines())\n",
    "        # Simple heuristic: <50=prototype, <200=functional, <500=production, 500+=best-in-class\n",
    "        if lines < 50:   score, maturity = 1, \"Prototype\"\n",
    "        elif lines < 200: score, maturity = 2, \"Functional\"\n",
    "        elif lines < 500: score, maturity = 3, \"Production\"\n",
    "        else:            score, maturity = 4, \"Best-in-Class\"\n",
    "    else:\n",
    "        lines, score, maturity = 0, 0, \"Missing\"\n",
    "    \n",
    "    maturity_data.append({\n",
    "        \"subsystem\": name, \"lines\": lines, \"score\": score, \"maturity\": maturity\n",
    "    })\n",
    "\n",
    "df_maturity = pd.DataFrame(maturity_data).sort_values(\"score\")\n",
    "\n",
    "color_map = {\n",
    "    \"Missing\": \"#ef5350\", \"Prototype\": \"#ff7043\",\n",
    "    \"Functional\": \"#ffa726\", \"Production\": \"#26a69a\", \"Best-in-Class\": \"#42a5f5\"\n",
    "}\n",
    "\n",
    "fig = px.bar(\n",
    "    df_maturity, x=\"score\", y=\"subsystem\", orientation=\"h\",\n",
    "    color=\"maturity\", color_discrete_map=color_map,\n",
    "    title=\"Subsystem Maturity Scores (Engineer Agent Assessment)\",\n",
    "    labels={\"score\": \"Maturity Score (0-4)\", \"subsystem\": \"Subsystem\"},\n",
    "    range_x=[0, 4.5],\n",
    "    text=\"lines\",\n",
    "    hover_data=[\"lines\", \"maturity\"]\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text} lines', textposition='outside')\n",
    "fig.update_layout(height=450)\n",
    "fig.show()\n",
    "\n",
    "avg = df_maturity[\"score\"].mean()\n",
    "print(f\"\\nAverage maturity: {avg:.2f}/4 ({['Below Prototype','Prototype','Functional','Production','Best-in-Class'][round(avg)]})\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sentinel Agent â€” QA Validation\n",
    "\n",
    "The Sentinel runs the test suite and linter, calculates a confidence score (1-10),\n",
    "and routes the result: â‰¥7 â†’ human review, <7 â†’ back to Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  SENTINEL AGENT â€” QA Validation\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "sentinel_result = agents[\"sentinel\"].run(changed_files=[])\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if sentinel_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if sentinel_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {sentinel_result.summary}\")\n",
    "\n",
    "if sentinel_result.metrics:\n",
    "    tests = sentinel_result.metrics.get(\"tests\", {})\n",
    "    lint = sentinel_result.metrics.get(\"lint\", {})\n",
    "    score = sentinel_result.metrics.get(\"confidence_score\", 0)\n",
    "\n",
    "    print(\"\\nğŸ§ª Test Results:\")\n",
    "    print(f\"   Passed:  {tests.get('passed', 0)}\")\n",
    "    print(f\"   Failed:  {tests.get('failed', 0)}\")\n",
    "    print(f\"   Errors:  {tests.get('errors', 0)}\")\n",
    "    print(f\"   Total:   {tests.get('total', 0)}\")\n",
    "    print(f\"   All OK:  {tests.get('all_passed', False)}\")\n",
    "\n",
    "    print(\"\\nğŸ” Lint Results:\")\n",
    "    print(f\"   Issues:  {lint.get('issues', '?')}\")\n",
    "    print(f\"   Clean:   {lint.get('clean', False)}\")\n",
    "\n",
    "    bar = \"â–ˆ\" * score + \"â–‘\" * (10 - score)\n",
    "    verdict = \"âœ… Ready for human review\" if score >= 7 else \"âš ï¸ Needs iteration\"\n",
    "    print(f\"\\nğŸ¯ Confidence Score: {bar} {score}/10\")\n",
    "    print(f\"   Verdict: {verdict}\")\n",
    "\n",
    "if sentinel_result.escalations:\n",
    "    print(\"\\nğŸ“‹ Escalations:\")\n",
    "    for esc in sentinel_result.escalations:\n",
    "        emoji = {\"urgent\": \"ğŸ”´\", \"review\": \"ğŸŸ¡\", \"fyi\": \"ğŸŸ¢\", \"metric\": \"ğŸ“Š\"}.get(esc.priority.value, \"â€¢\")\n",
    "        print(f\"   {emoji} [{esc.priority.value.upper()}] {esc.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confidence scoring breakdown\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "score = sentinel_result.metrics.get(\"confidence_score\", 0) if sentinel_result.metrics else 0\n",
    "tests = sentinel_result.metrics.get(\"tests\", {\"passed\": 0, \"failed\": 0, \"errors\": 0, \"total\": 0}) if sentinel_result.metrics else {}\n",
    "lint = sentinel_result.metrics.get(\"lint\", {\"issues\": 0, \"clean\": True}) if sentinel_result.metrics else {}\n",
    "\n",
    "fig = go.Figure(go.Indicator(\n",
    "    mode=\"gauge+number\",\n",
    "    value=score,\n",
    "    domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    title={'text': \"Sentinel Confidence Score\"},\n",
    "    gauge={\n",
    "        'axis': {'range': [0, 10]},\n",
    "        'bar': {'color': \"#42a5f5\" if score >= 7 else \"#ffa726\" if score >= 4 else \"#ef5350\"},\n",
    "        'steps': [\n",
    "            {'range': [0, 4], 'color': \"#ffebee\"},\n",
    "            {'range': [4, 7], 'color': \"#fff3e0\"},\n",
    "            {'range': [7, 10], 'color': \"#e8f5e9\"}\n",
    "        ],\n",
    "        'threshold': {\n",
    "            'line': {'color': \"green\", 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 7\n",
    "        }\n",
    "    }\n",
    "))\n",
    "fig.update_layout(height=350)\n",
    "fig.show()\n",
    "\n",
    "# Test result breakdown\n",
    "total = tests.get('total', 0)\n",
    "if total > 0:\n",
    "    fig2 = go.Figure(data=[go.Pie(\n",
    "        labels=['Passed', 'Failed', 'Errors'],\n",
    "        values=[tests.get('passed', 0), tests.get('failed', 0), tests.get('errors', 0)],\n",
    "        hole=0.4,\n",
    "        marker_colors=['#66bb6a', '#ef5350', '#ffa726']\n",
    "    )])\n",
    "    fig2.update_layout(title=f'Test Results ({total} total)', height=350)\n",
    "    fig2.show()\n",
    "else:\n",
    "    print(\"(No tests found in project â€” run 'pytest' to populate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Advocate Agent â€” GitHub Issue Triage\n",
    "\n",
    "The Advocate scans GitHub issues, classifies them (bug/feature/question/docs/security),\n",
    "auto-labels them, generates responses for questions, and flags stale issues.\n",
    "\n",
    "In dry-run mode, it reports what it **would** do without making any API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  ADVOCATE AGENT â€” GitHub Issue Triage\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "advocate_result = agents[\"advocate\"].run()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if advocate_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if advocate_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {advocate_result.summary}\")\n",
    "\n",
    "if advocate_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in advocate_result.metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in advocate_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if advocate_result.escalations:\n",
    "    print(\"\\nâš ï¸  Escalations:\")\n",
    "    for esc in advocate_result.escalations:\n",
    "        emoji = {\"urgent\": \"ğŸ”´\", \"review\": \"ğŸŸ¡\", \"fyi\": \"ğŸŸ¢\", \"metric\": \"ğŸ“Š\"}.get(esc.priority.value, \"â€¢\")\n",
    "        print(f\"   {emoji} [{esc.priority.value.upper()}] {esc.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Show how the Advocate classifies issues\n",
    "print(\"\\n  Issue Classifier Demo\")\n",
    "print(\"â”€\" * 60)\n",
    "\n",
    "sample_issues = [\n",
    "    {\"title\": \"App crashes when uploading large CSV files\",\n",
    "     \"body\": \"Getting a traceback: MemoryError when I upload files > 500MB\"},\n",
    "    {\"title\": \"Feature request: add support for Parquet format\",\n",
    "     \"body\": \"Would be great to support .parquet files in addition to CSV and Excel\"},\n",
    "    {\"title\": \"How do I configure the local LLM?\",\n",
    "     \"body\": \"I have a GGUF model downloaded but I'm not sure how to point Minerva to it\"},\n",
    "    {\"title\": \"SQL injection vulnerability in nl_query endpoint\",\n",
    "     \"body\": \"The nl_query endpoint doesn't sanitize inputs and is vulnerable to SQL injection\"},\n",
    "    {\"title\": \"Documentation for API discovery agent\",\n",
    "     \"body\": \"The README doesn't explain how the API discovery works, need examples\"},\n",
    "]\n",
    "\n",
    "# Use the Advocate's classifier directly\n",
    "for issue in sample_issues:\n",
    "    issue_type = agents[\"advocate\"]._classify_issue(issue[\"title\"], issue[\"body\"])\n",
    "    type_info = {\n",
    "        \"bug\": (\"ğŸ›\", \"Bug\"),\n",
    "        \"feature\": (\"âœ¨\", \"Feature Request\"),\n",
    "        \"question\": (\"â“\", \"Question\"),\n",
    "        \"docs\": (\"ğŸ“š\", \"Documentation\"),\n",
    "        \"security\": (\"ğŸ”’\", \"Security\"),\n",
    "    }.get(issue_type, (\"â€¢\", issue_type))\n",
    "    \n",
    "    print(f\"  {type_info[0]} [{type_info[1]:18s}] {issue['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Telemetry Agent â€” Usage Analytics\n",
    "\n",
    "The Telemetry Agent aggregates LLM interaction metrics, agent health data, and upload counts.\n",
    "It writes insights to the knowledge base for the Productizer to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  TELEMETRY AGENT â€” Usage Analytics\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "telemetry_result = agents[\"telemetry\"].run(days=7)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if telemetry_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if telemetry_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {telemetry_result.summary}\")\n",
    "\n",
    "if telemetry_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in telemetry_result.metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"   {k}: {v:.2%}\" if \"rate\" in k else f\"   {k}: {v:.1f}\")\n",
    "        else:\n",
    "            print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in telemetry_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if telemetry_result.escalations:\n",
    "    print(\"\\nâš ï¸  Escalations:\")\n",
    "    for esc in telemetry_result.escalations:\n",
    "        emoji = {\"urgent\": \"ğŸ”´\", \"review\": \"ğŸŸ¡\", \"fyi\": \"ğŸŸ¢\", \"metric\": \"ğŸ“Š\"}.get(esc.priority.value, \"â€¢\")\n",
    "        print(f\"   {emoji} [{esc.priority.value.upper()}] {esc.title}\")\n",
    "\n",
    "# Check if telemetry KB artifact was written\n",
    "telemetry_kb = PROJECT_ROOT / \"agents\" / \"knowledge_base\" / \"operations\" / \"telemetry_insights.md\"\n",
    "if telemetry_kb.exists():\n",
    "    print(f\"\\nğŸ“ KB artifact written: {telemetry_kb.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show agent health dashboard (from Telemetry's data)\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "state_dir = PROJECT_ROOT / \"agents\" / \"state\"\n",
    "known_agents = [\"conductor\", \"engineer\", \"sentinel\", \"advocate\",\n",
    "                \"productizer\", \"marketing\", \"support\", \"telemetry\"]\n",
    "\n",
    "health_data = []\n",
    "for agent_name in known_agents:\n",
    "    db_path = state_dir / f\"{agent_name}.db\"\n",
    "    if db_path.exists():\n",
    "        try:\n",
    "            with sqlite3.connect(str(db_path)) as conn:\n",
    "                row = conn.execute(\n",
    "                    \"SELECT COUNT(*), SUM(success), AVG(duration_seconds) FROM runs\"\n",
    "                ).fetchone()\n",
    "                runs = row[0] or 0\n",
    "                success = row[1] or 0\n",
    "                avg_dur = row[2] or 0\n",
    "                health_data.append({\n",
    "                    \"agent\": agent_name, \"runs\": runs,\n",
    "                    \"success_rate\": round(success / runs, 2) if runs else 0,\n",
    "                    \"avg_duration\": round(avg_dur, 1), \"db_exists\": True\n",
    "                })\n",
    "        except Exception:\n",
    "            health_data.append({\"agent\": agent_name, \"runs\": 0, \"success_rate\": 0,\n",
    "                                \"avg_duration\": 0, \"db_exists\": True})\n",
    "    else:\n",
    "        health_data.append({\"agent\": agent_name, \"runs\": 0, \"success_rate\": 0,\n",
    "                            \"avg_duration\": 0, \"db_exists\": False})\n",
    "\n",
    "df_health = pd.DataFrame(health_data)\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Total Runs', x=df_health['agent'], y=df_health['runs'],\n",
    "           marker_color='#42a5f5'),\n",
    "    go.Bar(name='Success Rate (%)', x=df_health['agent'],\n",
    "           y=df_health['success_rate'] * 100, marker_color='#66bb6a'),\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Agent Health Dashboard',\n",
    "    barmode='group',\n",
    "    xaxis_title='Agent',\n",
    "    yaxis_title='Count / Percentage',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "print(df_health.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Productizer Agent â€” Vertical Sales Kit\n",
    "\n",
    "The Productizer analyzes our demo retail dataset and generates a complete sales kit:\n",
    "buyer persona, pain points, elevator pitch, demo script, and ROI metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  PRODUCTIZER AGENT â€” Sales Kit Generator\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "# Pass the demo CSV we created earlier\n",
    "productizer_result = agents[\"productizer\"].run(file=demo_csv_path)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if productizer_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if productizer_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {productizer_result.summary}\")\n",
    "\n",
    "if productizer_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in productizer_result.metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in productizer_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if productizer_result.error:\n",
    "    print(f\"\\nâš ï¸  Error: {productizer_result.error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated sales kit\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "sales_kit_dir = PROJECT_ROOT / \"agents\" / \"knowledge_base\" / \"product\" / \"sales_kits\"\n",
    "kit_files = sorted(sales_kit_dir.glob(\"*.md\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "\n",
    "if kit_files:\n",
    "    latest_kit = kit_files[0]\n",
    "    content = latest_kit.read_text(encoding=\"utf-8\")\n",
    "    print(f\"ğŸ“„ Latest Sales Kit: {latest_kit.name} ({len(content)} chars)\")\n",
    "    print(\"â”€\" * 60)\n",
    "    display(Markdown(content))\n",
    "else:\n",
    "    print(\"(No sales kit found â€” LLM may be unavailable)\")\n",
    "    print(\"\\nExpected kit structure:\")\n",
    "    print(\"\"\"\n",
    "    # Minerva Sales Kit: Retail & E-Commerce\n",
    "    ## 1. The Executive Pitch\n",
    "    ## 2. Pain Points (Why they buy)\n",
    "    ## 3. ROI & Business Impact\n",
    "    ## 4. The Golden Demo Script\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Marketing Agent â€” Social Media Drafts\n",
    "\n",
    "The Marketing Agent reads recent git commits, pulls from the Productizer's knowledge base,\n",
    "and drafts platform-specific posts for HN, Reddit, and Twitter/X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  MARKETING AGENT â€” Social Media Drafts\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "marketing_result = agents[\"marketing\"].run(days=7)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if marketing_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if marketing_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {marketing_result.summary}\")\n",
    "\n",
    "if marketing_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in marketing_result.metrics.items():\n",
    "        if k != 'saved_paths':\n",
    "            print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in marketing_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if marketing_result.escalations:\n",
    "    print(f\"\\nâœ… {len(marketing_result.escalations)} draft(s) escalated for review:\")\n",
    "    for esc in marketing_result.escalations:\n",
    "        print(f\"   â†’ {esc.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated marketing drafts\n",
    "marketing_dir = PROJECT_ROOT / \"agents\" / \"digests\" / \"marketing\"\n",
    "marketing_files = sorted(marketing_dir.glob(\"*.md\"), key=lambda p: p.stat().st_mtime, reverse=True) if marketing_dir.exists() else []\n",
    "\n",
    "if marketing_files:\n",
    "    print(f\"ğŸ“„ Generated {len(marketing_files)} draft(s):\")\n",
    "    for draft_file in marketing_files[:3]:\n",
    "        platform = draft_file.stem.split(\"_\", 1)[-1].replace(\"_\", \" \").title()\n",
    "        content = draft_file.read_text(encoding=\"utf-8\")\n",
    "        print(f\"\\n{'â”€'*60}\")\n",
    "        print(f\"  {platform}: {draft_file.name}\")\n",
    "        print(f\"{'â”€'*60}\")\n",
    "        print(content[:600] + (\"...\" if len(content) > 600 else \"\"))\n",
    "else:\n",
    "    print(\"(No marketing drafts found â€” LLM may be unavailable or no commits in last 7 days)\")\n",
    "\n",
    "# Show recent git commits the agent used\n",
    "import subprocess\n",
    "try:\n",
    "    output = subprocess.check_output(\n",
    "        [\"git\", \"log\", \"--since=7.days.ago\", \"--pretty=format:%h %s\", \"--no-merges\"],\n",
    "        cwd=str(PROJECT_ROOT), stderr=subprocess.DEVNULL, timeout=10\n",
    "    ).decode(\"utf-8\").strip()\n",
    "    if output:\n",
    "        print(\"\\nğŸ“¦ Recent commits used as source:\")\n",
    "        for line in output.splitlines()[:5]:\n",
    "            print(f\"   {line}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Support Agent â€” FAQ-Backed Responder\n",
    "\n",
    "The Support Agent handles incoming questions against a structured FAQ + vector-store lookup.\n",
    "High-confidence matches get auto-responded; low-confidence ones escalate to human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  SUPPORT AGENT â€” FAQ-Backed Responder\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "support_questions = [\n",
    "    \"How do I upload a CSV file to Minerva?\",\n",
    "    \"Can Minerva handle Excel files?\",\n",
    "    \"What LLM models does Minerva support?\",\n",
    "    \"My analysis keeps failing with a timeout error\",\n",
    "    \"How do I export results to PDF?\",\n",
    "]\n",
    "\n",
    "for question in support_questions:\n",
    "    start = time.time()\n",
    "    result = agents[\"support\"].run(question=question)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nâ“ Q: {question}\")\n",
    "    \n",
    "    if result.escalations:\n",
    "        for esc in result.escalations:\n",
    "            emoji = {\"urgent\": \"ğŸ”´\", \"review\": \"ğŸŸ¡\", \"fyi\": \"ğŸŸ¢\", \"metric\": \"ğŸ“Š\"}.get(esc.priority.value, \"â€¢\")\n",
    "            if esc.priority.value == \"fyi\":\n",
    "                # Auto-responded\n",
    "                response = esc.detail[:300]\n",
    "                print(f\"   ğŸŸ¢ Auto-responded: {response[:200]}...\" if len(response) > 200 else f\"   ğŸŸ¢ Auto-responded: {response}\")\n",
    "            else:\n",
    "                print(f\"   {emoji} Escalated for human review: {esc.title}\")\n",
    "    else:\n",
    "        print(f\"   â„¹ï¸  {result.summary}\")\n",
    "    \n",
    "    print(f\"   â±ï¸  {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few FAQ entries to populate the knowledge base for future questions\n",
    "from agents.support import SupportAgent\n",
    "\n",
    "faq_entries = [\n",
    "    {\n",
    "        \"question\": \"How do I upload a CSV file?\",\n",
    "        \"answer\": \"In the Streamlit dashboard, go to the 'Data Upload' tab and drag-and-drop your CSV. \"\n",
    "                 \"Files up to 200MB are supported. Run `minerva dashboard` to start the UI.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What LLM models does Minerva support?\",\n",
    "        \"answer\": \"Minerva supports: Anthropic Claude (cloud), OpenAI GPT models (cloud), \"\n",
    "                 \"local GGUF models via llama.cpp, and Ollama models. Configure via the LLM Settings tab.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I run Minerva in offline mode?\",\n",
    "        \"answer\": \"Download a GGUF model file and place it in adm/llm_backends/local_model/. \"\n",
    "                 \"Set ENABLE_LOCAL_LLM=1 in your .env file. Minerva will auto-detect and use it.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I export results to PDF?\",\n",
    "        \"answer\": \"Use the 'Export' button in the dashboard, or run `minerva analyze data.csv --export pdf`. \"\n",
    "                 \"Reports are saved to the reports/ directory.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for entry in faq_entries:\n",
    "    agents[\"support\"].add_faq_entry(entry[\"question\"], entry[\"answer\"])\n",
    "\n",
    "print(f\"âœ… Added {len(faq_entries)} FAQ entries\")\n",
    "print(f\"   FAQ size: {len(agents['support']._faq)} entries\")\n",
    "\n",
    "# Re-test with a question that now matches the FAQ\n",
    "result = agents[\"support\"].run(question=\"How do I upload files to Minerva?\")\n",
    "if result.escalations:\n",
    "    esc = result.escalations[0]\n",
    "    print(f\"\\nâ“ Q: How do I upload files to Minerva?\")\n",
    "    print(f\"   {'ğŸŸ¢ Auto-responded' if esc.priority.value == 'fyi' else 'ğŸŸ¡ Escalated'}: {esc.detail[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Presentation Agent â€” PowerPoint Generator\n",
    "\n",
    "The Presentation Agent takes the latest Markdown Sales Kit and converts it\n",
    "into a polished `.pptx` file ready for client meetings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  PRESENTATION AGENT â€” PowerPoint Generator\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "presentation_result = agents[\"presentation\"].run()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if presentation_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if presentation_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {presentation_result.summary}\")\n",
    "\n",
    "if presentation_result.metrics:\n",
    "    print(\"\\nğŸ“Š Metrics:\")\n",
    "    for k, v in presentation_result.metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Actions taken:\")\n",
    "for action in presentation_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "if presentation_result.error:\n",
    "    print(f\"\\nâš ï¸  Error: {presentation_result.error}\")\n",
    "\n",
    "# Check if .pptx was created\n",
    "pptx_dir = PROJECT_ROOT / \"reports\" / \"presentations\"\n",
    "if pptx_dir.exists():\n",
    "    pptx_files = sorted(pptx_dir.glob(\"*.pptx\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if pptx_files:\n",
    "        print(f\"\\nğŸ“Š Presentation saved: {pptx_files[0].name}\")\n",
    "        print(f\"   Size: {pptx_files[0].stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Conductor Agent â€” Full Orchestration\n",
    "\n",
    "The Conductor is the daily orchestrator. It:\n",
    "1. Scans GitHub for repo stats\n",
    "2. Runs all enabled sub-agents\n",
    "3. Aggregates escalations by priority (ğŸ”´ Urgent / ğŸŸ¡ Review / ğŸŸ¢ FYI / ğŸ“Š Metric)\n",
    "4. Compiles and saves the daily digest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â•\" * 60)\n",
    "print(\"  CONDUCTOR AGENT â€” Full Orchestration\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "conductor = ConductorAgent(config=make_config(\"conductor\"))\n",
    "\n",
    "start = time.time()\n",
    "conductor_result = conductor.run()\n",
    "elapsed = time.time() - start\n",
    "\n",
    "status = \"âœ…\" if conductor_result.success else \"âŒ\"\n",
    "print(f\"\\n{status} Status: {'Success' if conductor_result.success else 'Failed'}\")\n",
    "print(f\"â±ï¸  Duration: {elapsed:.1f}s\")\n",
    "print(f\"ğŸ“ Summary: {conductor_result.summary}\")\n",
    "\n",
    "if conductor_result.metrics:\n",
    "    print(\"\\nğŸ“Š GitHub Metrics:\")\n",
    "    for k, v in conductor_result.metrics.items():\n",
    "        print(f\"   {k}: {v}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Conductor actions:\")\n",
    "for action in conductor_result.actions_taken:\n",
    "    print(f\"   â†’ {action}\")\n",
    "\n",
    "# Tally escalations by priority\n",
    "from collections import Counter\n",
    "priority_counts = Counter(esc.priority.value for esc in conductor_result.escalations)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Escalation Summary ({len(conductor_result.escalations)} total):\")\n",
    "print(f\"   ğŸ”´ Urgent:  {priority_counts.get('urgent', 0)}\")\n",
    "print(f\"   ğŸŸ¡ Review:  {priority_counts.get('review', 0)}\")\n",
    "print(f\"   ğŸŸ¢ FYI:     {priority_counts.get('fyi', 0)}\")\n",
    "print(f\"   ğŸ“Š Metric:  {priority_counts.get('metric', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the daily digest\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "\n",
    "digest_dir = PROJECT_ROOT / \"agents\" / \"digests\"\n",
    "today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "digest_path = digest_dir / f\"{today}.md\"\n",
    "\n",
    "if digest_path.exists():\n",
    "    content = digest_path.read_text(encoding=\"utf-8\")\n",
    "    print(f\"ğŸ“„ Daily Digest: {digest_path.name} ({len(content)} chars)\")\n",
    "    print(\"â”€\" * 60)\n",
    "    display(Markdown(content))\n",
    "else:\n",
    "    # Show available digests\n",
    "    digests = sorted(digest_dir.glob(\"*.md\"), reverse=True) if digest_dir.exists() else []\n",
    "    if digests:\n",
    "        latest = digests[0]\n",
    "        content = latest.read_text(encoding=\"utf-8\")\n",
    "        print(f\"ğŸ“„ Latest Digest: {latest.name}\")\n",
    "        display(Markdown(content))\n",
    "    else:\n",
    "        print(\"(No digest found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Full Workforce Summary Dashboard\n",
    "\n",
    "Aggregated results from all agents in a single view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Collect all results\n",
    "all_results = {\n",
    "    \"engineer\":     engineer_result,\n",
    "    \"sentinel\":     sentinel_result,\n",
    "    \"advocate\":     advocate_result,\n",
    "    \"telemetry\":    telemetry_result,\n",
    "    \"productizer\":  productizer_result,\n",
    "    \"marketing\":    marketing_result,\n",
    "    \"support\":      agents[\"support\"].run(question=\"test\"),  # Quick test run\n",
    "    \"presentation\": presentation_result,\n",
    "    \"conductor\":    conductor_result,\n",
    "}\n",
    "\n",
    "# Build summary table\n",
    "summary_rows = []\n",
    "for name, result in all_results.items():\n",
    "    priority_counts = Counter(esc.priority.value for esc in result.escalations)\n",
    "    summary_rows.append({\n",
    "        \"Agent\": name.title(),\n",
    "        \"Status\": \"âœ…\" if result.success else \"âŒ\",\n",
    "        \"Actions\": len(result.actions_taken),\n",
    "        \"ğŸ”´\": priority_counts.get('urgent', 0),\n",
    "        \"ğŸŸ¡\": priority_counts.get('review', 0),\n",
    "        \"ğŸŸ¢\": priority_counts.get('fyi', 0),\n",
    "        \"ğŸ“Š\": priority_counts.get('metric', 0),\n",
    "        \"Duration (s)\": round(result.duration_seconds, 2),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Render as a styled table\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(\n",
    "        values=list(df_summary.columns),\n",
    "        fill_color='#1a237e',\n",
    "        font=dict(color='white', size=13),\n",
    "        align='left'\n",
    "    ),\n",
    "    cells=dict(\n",
    "        values=[df_summary[col] for col in df_summary.columns],\n",
    "        fill_color=[['#e8eaf6' if i % 2 == 0 else 'white' for i in range(len(df_summary))]],\n",
    "        align='left',\n",
    "        font=dict(size=12)\n",
    "    )\n",
    ")])\n",
    "fig.update_layout(title='Minerva Agent Workforce â€” Run Summary', height=420)\n",
    "fig.show()\n",
    "\n",
    "# Escalation breakdown pie chart\n",
    "total_urgent = sum(r.get('ğŸ”´', 0) for r in summary_rows)\n",
    "total_review = sum(r.get('ğŸŸ¡', 0) for r in summary_rows)\n",
    "total_fyi    = sum(r.get('ğŸŸ¢', 0) for r in summary_rows)\n",
    "total_metric = sum(r.get('ğŸ“Š', 0) for r in summary_rows)\n",
    "\n",
    "total_esc = total_urgent + total_review + total_fyi + total_metric\n",
    "if total_esc > 0:\n",
    "    fig2 = go.Figure(data=[go.Pie(\n",
    "        labels=['ğŸ”´ Urgent', 'ğŸŸ¡ Review', 'ğŸŸ¢ FYI', 'ğŸ“Š Metric'],\n",
    "        values=[total_urgent, total_review, total_fyi, total_metric],\n",
    "        hole=0.4,\n",
    "        marker_colors=['#ef5350', '#ffa726', '#66bb6a', '#42a5f5']\n",
    "    )])\n",
    "    fig2.update_layout(title=f'Escalation Distribution ({total_esc} total)', height=380)\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"â•\" * 60)\n",
    "print(\"  MINERVA AGENT WORKFORCE â€” FINAL SUMMARY\")\n",
    "print(\"â•\" * 60)\n",
    "\n",
    "success_count = sum(1 for r in all_results.values() if r.success)\n",
    "fail_count = len(all_results) - success_count\n",
    "total_actions = sum(len(r.actions_taken) for r in all_results.values())\n",
    "total_escalations = sum(len(r.escalations) for r in all_results.values())\n",
    "\n",
    "print(f\"\\n  Agents run:    {len(all_results)}\")\n",
    "print(f\"  Succeeded:     {success_count} âœ…\")\n",
    "print(f\"  Failed:        {fail_count} âŒ\")\n",
    "print(f\"  Total actions: {total_actions}\")\n",
    "print(f\"  Escalations:   {total_escalations}\")\n",
    "print(f\"    ğŸ”´ Urgent:   {total_urgent}\")\n",
    "print(f\"    ğŸŸ¡ Review:   {total_review}\")\n",
    "print(f\"    ğŸŸ¢ FYI:      {total_fyi}\")\n",
    "print(f\"    ğŸ“Š Metric:   {total_metric}\")\n",
    "print()\n",
    "print(\"  Knowledge Base outputs:\")\n",
    "print(\"    â†’ agents/knowledge_base/product/vision_gap_analysis.md\")\n",
    "print(\"    â†’ agents/knowledge_base/product/sales_kits/*.md\")\n",
    "print(\"    â†’ agents/knowledge_base/operations/telemetry_insights.md\")\n",
    "print(\"    â†’ agents/digests/*.md  (daily digest)\")\n",
    "print(\"    â†’ agents/digests/marketing/*.md  (social drafts)\")\n",
    "print(\"    â†’ agents/digests/telemetry/*.md  (telemetry report)\")\n",
    "print(\"    â†’ reports/presentations/*.pptx\")\n",
    "print()\n",
    "print(\"  Next steps:\")\n",
    "print(\"    1. Run `minerva dashboard` for the full Streamlit UI\")\n",
    "print(\"    2. Run `python -m agents run all` to run all agents via CLI\")\n",
    "print(\"    3. Review the daily digest in agents/digests/\")\n",
    "print(\"    4. Enable live mode in agents_config.yaml for production\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Agent Configuration Reference\n",
    "\n",
    "How to configure each agent for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "config_doc = \"\"\"\n",
    "## Configuration Guide\n",
    "\n",
    "### `agents/agents_config.yaml`\n",
    "\n",
    "```yaml\n",
    "agents:\n",
    "  conductor:\n",
    "    enabled: true          # Run the daily orchestration\n",
    "    schedule: daily\n",
    "    dry_run: false         # Set to false for LIVE mode\n",
    "    llm_model: claude-3-5-sonnet\n",
    "\n",
    "  engineer:\n",
    "    enabled: true\n",
    "    schedule: weekly       # Weekly gap analysis\n",
    "    dry_run: true\n",
    "\n",
    "  sentinel:\n",
    "    enabled: true\n",
    "    schedule: on_event     # Triggered by PR events\n",
    "    extra:\n",
    "      min_confidence_score: 7\n",
    "\n",
    "  marketing:\n",
    "    enabled: true          # Enable before public launch\n",
    "    schedule: weekly\n",
    "    extra:\n",
    "      days: 7              # Look-back window for git log\n",
    "\n",
    "  telemetry:\n",
    "    enabled: true          # Enable to track usage\n",
    "    schedule: weekly\n",
    "    extra:\n",
    "      error_rate_threshold: 0.30\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "| Variable | Purpose |\n",
    "|---|---|\n",
    "| `ANTHROPIC_API_KEY` | Enable Claude cloud LLM |\n",
    "| `GITHUB_TOKEN` | Enable Advocate & Sentinel GitHub access |\n",
    "| `AGENT_DRY_RUN=1` | Force ALL agents into dry-run (safe for testing) |\n",
    "| `ENABLE_LOCAL_LLM=1` | Use local GGUF model |\n",
    "| `MCP_ENABLED=1` | Start MCP server for Claude Desktop |\n",
    "\n",
    "### Running Agents\n",
    "\n",
    "```bash\n",
    "# Run all enabled agents\n",
    "python -m agents run all\n",
    "\n",
    "# Run a specific agent in dry-run mode\n",
    "python -m agents run engineer --dry-run\n",
    "\n",
    "# Trigger Sentinel on a PR\n",
    "python -m agents run sentinel --pr_number=42\n",
    "\n",
    "# Generate a sales kit for a topic\n",
    "python -m agents run productizer --topic=crypto\n",
    "\n",
    "# Ask the Support agent a question\n",
    "python -m agents run support --question=\"How do I configure Ollama?\"\n",
    "\n",
    "# List all agents and their status\n",
    "python -m agents list\n",
    "```\n",
    "\n",
    "### Conductor Scheduling\n",
    "\n",
    "The Conductor runs daily and triggers:\n",
    "- **Advocate** â†’ daily GitHub triage\n",
    "- **Engineer** â†’ weekly gap analysis (Mondays)\n",
    "- **Telemetry** â†’ weekly usage metrics (Sundays)\n",
    "- **Marketing** â†’ weekly social drafts\n",
    "\n",
    "Use `agents/scheduler.py` for cron-based execution or integrate with GitHub Actions.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(config_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the agents_config.yaml contents\n",
    "config_path = PROJECT_ROOT / \"agents\" / \"agents_config.yaml\"\n",
    "if config_path.exists():\n",
    "    import yaml\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Current agents_config.yaml:\")\n",
    "    print(\"â”€\" * 60)\n",
    "    for agent_name, settings in config.get(\"agents\", {}).items():\n",
    "        enabled = settings.get('enabled', False)\n",
    "        dry_run = settings.get('dry_run', True)\n",
    "        schedule = settings.get('schedule', '?')\n",
    "        mode = \"ğŸŸ¢ LIVE\" if (enabled and not dry_run) else (\"ğŸŸ¡ dry-run\" if enabled else \"â¸  disabled\")\n",
    "        print(f\"  {agent_name:15s} {mode:18s} schedule={schedule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. MCP Server â€” Tool Exposure for Claude Desktop\n",
    "\n",
    "Minerva exposes all its tools via the Model Context Protocol (MCP),\n",
    "making them available to Claude Desktop and other AI clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show MCP server configuration\n",
    "mcp_config_path = PROJECT_ROOT / \"mcp.json\"\n",
    "\n",
    "if mcp_config_path.exists():\n",
    "    with open(mcp_config_path) as f:\n",
    "        mcp_config = json.load(f)\n",
    "    print(\"mcp.json (Claude Desktop configuration):\")\n",
    "    print(json.dumps(mcp_config, indent=2))\n",
    "\n",
    "# Show API registry size\n",
    "try:\n",
    "    from mcp_server.api_registry import API_REGISTRY\n",
    "    print(f\"\\n\\nğŸ“¡ API Registry: {len(API_REGISTRY)} public APIs registered\")\n",
    "    categories = {}\n",
    "    for api_id, api in API_REGISTRY.items():\n",
    "        cat = api.get('category', 'Other')\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    for cat, count in sorted(categories.items(), key=lambda x: -x[1])[:10]:\n",
    "        bar = 'â–ˆ' * min(count, 20)\n",
    "        print(f\"   {cat:25s} {bar} {count}\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nMCP Server modules: {e}\")\n",
    "    print(\"Run `python -m mcp_server run` to start the MCP server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: API Discovery\n",
    "try:\n",
    "    from mcp_server.discovery_agent import APIDiscoveryAgent\n",
    "    \n",
    "    print(\"â•\" * 60)\n",
    "    print(\"  MCP: API Discovery Demo\")\n",
    "    print(\"â•\" * 60)\n",
    "    \n",
    "    discoverer = APIDiscoveryAgent()\n",
    "    \n",
    "    demo_queries = [\n",
    "        \"US economic GDP data\",\n",
    "        \"cryptocurrency prices\",\n",
    "        \"weather forecast\",\n",
    "    ]\n",
    "    \n",
    "    for query in demo_queries:\n",
    "        result = discoverer.discover_api(query)\n",
    "        print(f\"\\n  Query: '{query}'\")\n",
    "        if isinstance(result, dict):\n",
    "            print(f\"  â†’ Found: {result.get('name', result.get('api_id', 'unknown'))}\")\n",
    "            if result.get('endpoint'):\n",
    "                print(f\"  â†’ Endpoint: {result.get('endpoint')[:60]}\")\n",
    "        elif isinstance(result, list) and result:\n",
    "            print(f\"  â†’ {len(result)} matches found\")\n",
    "            for r in result[:2]:\n",
    "                if isinstance(r, dict):\n",
    "                    print(f\"     â€¢ {r.get('name', r.get('api_id', str(r)[:40]))}\")\n",
    "        else:\n",
    "            print(f\"  â†’ {str(result)[:80]}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"API Discovery not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix: Escalation Priority Reference\n",
    "\n",
    "| Priority | Emoji | Meaning | Response Time |\n",
    "|----------|-------|---------|---------------|\n",
    "| URGENT   | ğŸ”´    | Requires human action today | Same day |\n",
    "| REVIEW   | ğŸŸ¡    | Needs review within 48 hours | 48 hours |\n",
    "| FYI      | ğŸŸ¢    | Handled autonomously, informational | None needed |\n",
    "| METRIC   | ğŸ“Š    | Data point for metrics snapshot | None needed |\n",
    "\n",
    "## Data Flow Between Agents\n",
    "\n",
    "```\n",
    "Telemetry â”€â”€â†’ knowledge_base/operations/telemetry_insights.md\n",
    "    â”‚                    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Productizer â”€â”€â†’ knowledge_base/product/sales_kits/*.md\n",
    "                                              â”‚\n",
    "                              Marketing â†â”€â”€â”€â”€â”€â”˜ (reads sales kits)\n",
    "                                  â”‚\n",
    "                                  â””â”€â”€â†’ digests/marketing/*.md (social drafts)\n",
    "\n",
    "Engineer â”€â”€â†’ knowledge_base/product/vision_gap_analysis.md\n",
    "\n",
    "Productizer â”€â”€â†’ knowledge_base/product/sales_kits/*.md\n",
    "                        â”‚\n",
    "            Presentation â”€â”€â†’ reports/presentations/*.pptx\n",
    "\n",
    "Conductor â”€â”€â†’ agents/digests/YYYY-MM-DD.md (daily digest)\n",
    "```\n",
    "\n",
    "---\n",
    "*Generated by Minerva Agent Demo Notebook â€” run `minerva dashboard` for the full interactive experience.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat": 4,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}